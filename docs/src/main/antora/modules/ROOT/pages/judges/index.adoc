= AI Judges and Evaluation
:page-partial:

This section covers AI-powered evaluation and quality assessment for Spring AI Watsonx.ai applications.

== Overview

AI Judges are specialized models or systems that evaluate the quality, accuracy, and appropriateness of AI-generated content. They help ensure your AI applications meet quality standards and perform as expected.

== Evaluation Concepts

=== Response Quality

Assess the quality of model responses:

**Relevance**: Does the response address the query?

**Accuracy**: Is the information correct?

**Completeness**: Does it cover all aspects?

**Coherence**: Is it well-structured and logical?

=== Evaluation Metrics

Common metrics for AI evaluation:

**Semantic Similarity**

Compare response to expected output:
[source,java]
----
double similarity = evaluator.semanticSimilarity(
    actualResponse, 
    expectedResponse
);
----

**Factual Accuracy**

Verify factual claims:
[source,java]
----
boolean isAccurate = evaluator.checkFactualAccuracy(
    response, 
    knowledgeBase
);
----

**Toxicity Detection**

Check for harmful content:
[source,java]
----
ToxicityScore score = evaluator.analyzeToxicity(response);
----

== Evaluation Strategies

=== Model-Based Evaluation

Use AI models to evaluate AI outputs:

[source,java]
----
@Service
public class ModelBasedEvaluator {
    
    private final ChatModel judgeModel;
    
    public EvaluationResult evaluate(String prompt, String response) {
        String evaluationPrompt = String.format("""
            Evaluate the following AI response:
            
            Prompt: %s
            Response: %s
            
            Rate on a scale of 1-10 for:
            - Relevance
            - Accuracy
            - Completeness
            - Clarity
            
            Provide scores and brief justification.
            """, prompt, response);
        
        String evaluation = judgeModel.call(evaluationPrompt);
        return parseEvaluation(evaluation);
    }
}
----

=== Rule-Based Evaluation

Define explicit rules for evaluation:

[source,java]
----
@Service
public class RuleBasedEvaluator {
    
    public EvaluationResult evaluate(String response) {
        EvaluationResult result = new EvaluationResult();
        
        // Check length
        if (response.length() < 50) {
            result.addIssue("Response too short");
        }
        
        // Check for required keywords
        if (!containsRequiredKeywords(response)) {
            result.addIssue("Missing required information");
        }
        
        // Check formatting
        if (!isWellFormatted(response)) {
            result.addIssue("Poor formatting");
        }
        
        return result;
    }
}
----

=== Human-in-the-Loop

Combine automated and human evaluation:

[source,java]
----
@Service
public class HybridEvaluator {
    
    private final ModelBasedEvaluator autoEvaluator;
    private final HumanReviewQueue reviewQueue;
    
    public EvaluationResult evaluate(String prompt, String response) {
        // Automated evaluation
        EvaluationResult autoResult = autoEvaluator.evaluate(prompt, response);
        
        // Queue for human review if uncertain
        if (autoResult.getConfidence() < 0.8) {
            reviewQueue.add(new ReviewTask(prompt, response, autoResult));
        }
        
        return autoResult;
    }
}
----

== Testing and Validation

=== Unit Testing

Test individual components:

[source,java]
----
@Test
void shouldGenerateRelevantResponse() {
    String prompt = "What is Spring AI?";
    String response = chatModel.call(prompt);
    
    assertThat(response).contains("Spring", "AI");
    assertThat(response.length()).isGreaterThan(50);
}
----

=== Integration Testing

Test end-to-end flows:

[source,java]
----
@SpringBootTest
class ChatIntegrationTest {
    
    @Autowired
    private ChatService chatService;
    
    @Test
    void shouldHandleComplexQuery() {
        String query = "Explain RAG in simple terms";
        String response = chatService.chat(query);
        
        assertThat(response).isNotEmpty();
        assertThat(response).containsIgnoringCase("retrieval");
        assertThat(response).containsIgnoringCase("generation");
    }
}
----

=== Benchmark Testing

Compare model performance:

[source,java]
----
@Service
public class BenchmarkService {
    
    public BenchmarkResult runBenchmark(List<TestCase> testCases) {
        BenchmarkResult result = new BenchmarkResult();
        
        for (TestCase testCase : testCases) {
            long startTime = System.currentTimeMillis();
            String response = chatModel.call(testCase.getPrompt());
            long duration = System.currentTimeMillis() - startTime;
            
            result.addResult(new TestResult(
                testCase,
                response,
                duration,
                evaluateQuality(testCase, response)
            ));
        }
        
        return result;
    }
}
----

== Quality Assurance

=== Automated QA Pipeline

[source]
----
Code Change → Unit Tests → Integration Tests → 
Quality Evaluation → Human Review → Deployment
----

=== Continuous Monitoring

Monitor production quality:

[source,java]
----
@Service
public class QualityMonitor {
    
    private final MeterRegistry meterRegistry;
    
    public void recordResponse(String prompt, String response) {
        // Record metrics
        meterRegistry.counter("ai.responses.total").increment();
        
        // Evaluate quality
        double quality = evaluateQuality(response);
        meterRegistry.gauge("ai.responses.quality", quality);
        
        // Alert on low quality
        if (quality < 0.7) {
            alertLowQuality(prompt, response, quality);
        }
    }
}
----

=== A/B Testing

Compare different approaches:

[source,java]
----
@Service
public class ABTestingService {
    
    public String chat(String prompt, String userId) {
        // Assign user to variant
        String variant = assignVariant(userId);
        
        ChatModel model = getModelForVariant(variant);
        String response = model.call(prompt);
        
        // Track metrics by variant
        trackMetrics(variant, prompt, response);
        
        return response;
    }
}
----

== Best Practices

=== Evaluation Guidelines

* **Define clear criteria**: Establish what "good" means
* **Use multiple metrics**: Don't rely on single measure
* **Include edge cases**: Test boundary conditions
* **Regular updates**: Refresh test cases periodically
* **Document findings**: Keep evaluation records

=== Quality Standards

* **Accuracy threshold**: Minimum 90% factual accuracy
* **Response time**: Under 5 seconds for most queries
* **Relevance score**: Above 0.8 on similarity scale
* **Safety checks**: Zero tolerance for harmful content

=== Continuous Improvement

* Monitor production metrics
* Collect user feedback
* Analyze failure cases
* Update evaluation criteria
* Refine prompts and models

== Tools and Frameworks

=== Evaluation Frameworks

**Spring AI Test Support**
[source,java]
----
@SpringAITest
class ChatModelTest {
    @Autowired
    private ChatModel chatModel;
    
    @Test
    void testChatModel() {
        // Test implementation
    }
}
----

**Custom Evaluation Framework**
[source,java]
----
@Configuration
public class EvaluationConfig {
    
    @Bean
    public EvaluationFramework evaluationFramework() {
        return EvaluationFramework.builder()
            .addEvaluator(new SemanticEvaluator())
            .addEvaluator(new FactualEvaluator())
            .addEvaluator(new SafetyEvaluator())
            .build();
    }
}
----

=== Monitoring Tools

* Prometheus for metrics
* Grafana for visualization
* ELK stack for logging
* Jaeger for tracing

== Future Enhancements

Planned improvements for evaluation:

* Automated test case generation
* Advanced similarity metrics
* Multi-dimensional quality scoring
* Real-time quality dashboards
* Predictive quality models

== See Also

* xref:concepts/index.adoc[Core Concepts]
* xref:samples.adoc[Sample Applications]
* link:https://spring.io/projects/spring-ai[Spring AI Documentation^]